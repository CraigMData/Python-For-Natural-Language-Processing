{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["<h1>Pre-Processing Text for Natural Language Processing</h1>\n","This notebook uses data from the Kaggle: Bag of Words meets Bags of Popcorn competition. It focuses on understanding the different methods of processing text based data into a form understandable by machine learning models.\n","\n","The task is to classify the sentiment of movie reviews using the IMBD Movie Review Dataset. Movie reviews can either be positive (1) or negative (0). Binary classification is typically a straight forward problem, hence this task allows us to focus on understanding the different pre-processing and tokenisation methods for text.  "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:27:59.524297Z","iopub.status.busy":"2024-03-17T13:27:59.523817Z","iopub.status.idle":"2024-03-17T13:28:00.677169Z","shell.execute_reply":"2024-03-17T13:28:00.676231Z","shell.execute_reply.started":"2024-03-17T13:27:59.524259Z"},"trusted":true},"outputs":[],"source":["# Import essential libraries\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import re\n","import os\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Bag of Words</h2>\n","The first method for processing text into numbers explored was the Bag of Words method. In this method, text is first processed to remove all but (typically) the words. The remaining words are then numbered. Usually, the vocabulary will be reduced to only the most frequently occuring N words to reduce the dimensionality of the problem. This method removes word order however maintains multiplicity.\n","\n","The logic for use of this method with the task at hand is that positive and negative reviews are likely to contain different types of words, for example, excellent, amazing, great compared to awful, boring, and terrible. For simplicity, we explore only words.\n","\n","To begin, we explore the datast."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:00.680101Z","iopub.status.busy":"2024-03-17T13:28:00.679311Z","iopub.status.idle":"2024-03-17T13:28:01.635615Z","shell.execute_reply":"2024-03-17T13:28:01.633849Z","shell.execute_reply.started":"2024-03-17T13:28:00.680065Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>sentiment</th>\n","      <th>review</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\"5814_8\"</td>\n","      <td>1</td>\n","      <td>\"With all this stuff going down at the moment ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\"2381_9\"</td>\n","      <td>1</td>\n","      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\"7759_3\"</td>\n","      <td>0</td>\n","      <td>\"The film starts with a manager (Nicholas Bell...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\"3630_4\"</td>\n","      <td>0</td>\n","      <td>\"It must be assumed that those who praised thi...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\"9495_8\"</td>\n","      <td>1</td>\n","      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>\"8196_8\"</td>\n","      <td>1</td>\n","      <td>\"I dont know why people think this is such a b...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>\"7166_2\"</td>\n","      <td>0</td>\n","      <td>\"This movie could have been very good, but com...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>\"10633_1\"</td>\n","      <td>0</td>\n","      <td>\"I watched this video at a friend's house. I'm...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>\"319_1\"</td>\n","      <td>0</td>\n","      <td>\"A friend of mine bought this film for £1, and...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>\"8713_10\"</td>\n","      <td>1</td>\n","      <td>\"&lt;br /&gt;&lt;br /&gt;This movie is full of references....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          id  sentiment                                             review\n","0   \"5814_8\"          1  \"With all this stuff going down at the moment ...\n","1   \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n","2   \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n","3   \"3630_4\"          0  \"It must be assumed that those who praised thi...\n","4   \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ...\n","5   \"8196_8\"          1  \"I dont know why people think this is such a b...\n","6   \"7166_2\"          0  \"This movie could have been very good, but com...\n","7  \"10633_1\"          0  \"I watched this video at a friend's house. I'm...\n","8    \"319_1\"          0  \"A friend of mine bought this film for £1, and...\n","9  \"8713_10\"          1  \"<br /><br />This movie is full of references...."]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Set the working directory path where the data files are located is specified.\n","Directory = r'D:\\TechWork\\NonConfidentialProjects\\NLPForMovieReviews'\n","\n","# Load the training dataset.\n","train_ds = pd.read_csv(os.path.join(Directory, \"labeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)\n","\n","# Load in testing dataset.\n","test_ds = pd.read_csv(os.path.join(Directory, \"testData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)\n","\n","# Print the first 10 rows of training dataset.\n","train_ds.head(10)"]},{"cell_type":"markdown","metadata":{},"source":["Before analysing the training dataset further, we separate it into a training and validating set."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:01.638316Z","iopub.status.busy":"2024-03-17T13:28:01.637628Z","iopub.status.idle":"2024-03-17T13:28:01.665512Z","shell.execute_reply":"2024-03-17T13:28:01.664131Z","shell.execute_reply.started":"2024-03-17T13:28:01.638270Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Dataset\n","Positive Samples = 0.50\n","Negative Samples = 0.50\n","\n","\n","Validate Dataset\n","Positive Samples = 0.50\n","Negative Samples = 0.50\n"]}],"source":["# Separate into training and testing dataset with a 70:30 split.\n","# Note the dataset is already shuffled.\n","train, validate = train_test_split(train_ds, test_size = 0.3, shuffle = False)\n","\n","# Investigate the split of positive and negative samples\n","print(f'Train Dataset')\n","print(f'Positive Samples = {sum(train[\"sentiment\"])/len(train):.2f}')\n","print(f'Negative Samples = {(len(train)-sum(train[\"sentiment\"]))/len(train):.2f}')\n","print('\\n')\n","print(f'Validate Dataset')\n","print(f'Positive Samples = {sum(validate[\"sentiment\"])/len(validate):.2f}')\n","print(f'Negative Samples = {(len(validate)-sum(validate[\"sentiment\"]))/len(validate):.2f}')"]},{"cell_type":"markdown","metadata":{},"source":["The classification of reviews is split evenly in each dataset, limiting the potential biases.\n","\n","To understand the better the content and format of the reviews, we read the first one."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:01.668512Z","iopub.status.busy":"2024-03-17T13:28:01.667105Z","iopub.status.idle":"2024-03-17T13:28:01.679597Z","shell.execute_reply":"2024-03-17T13:28:01.678389Z","shell.execute_reply.started":"2024-03-17T13:28:01.668458Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\"With all this stuff going down at the moment with MJ i\\'ve started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ\\'s feeling towards the press and also the obvious message of drugs are bad m\\'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci\\'s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ\\'s music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ\\'s bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i\\'ve gave this subject....hmmm well i don\\'t know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train['review'][0]"]},{"cell_type":"markdown","metadata":{},"source":["The review contains HTML tags such as '< br >', punctuation such as '...' and escape characters for apostrophes. \n","\n","To remove HTML related features, we use Beautiful Soup, and demonstrate its use on the first review."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:01.683402Z","iopub.status.busy":"2024-03-17T13:28:01.683000Z","iopub.status.idle":"2024-03-17T13:28:01.839460Z","shell.execute_reply":"2024-03-17T13:28:01.837936Z","shell.execute_reply.started":"2024-03-17T13:28:01.683370Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"]}],"source":["from bs4 import BeautifulSoup\n","\n","# Create a Beautiful Soup object \n","sample_1 = BeautifulSoup(train[\"review\"][0])  \n","\n","# Print the processed review\n","print(sample_1.get_text())"]},{"cell_type":"markdown","metadata":{},"source":["For simplicity, we then removes all punctuation. We follow this and also immediately separate the dataset into individual words containing only lower case letters."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:01.841489Z","iopub.status.busy":"2024-03-17T13:28:01.840848Z","iopub.status.idle":"2024-03-17T13:28:01.850055Z","shell.execute_reply":"2024-03-17T13:28:01.848662Z","shell.execute_reply.started":"2024-03-17T13:28:01.841455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again', 'maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent', 'moonwalker', 'is', 'part', 'biography', 'part', 'feature', 'film', 'which', 'i', 'remember', 'going', 'to', 'see', 'at', 'the', 'cinema', 'when', 'it', 'was', 'originally', 'released', 'some', 'of', 'it', 'has', 'subtle', 'messages', 'about', 'mj', 's', 'feeling', 'towards', 'the', 'press', 'and', 'also', 'the', 'obvious', 'message', 'of', 'drugs', 'are', 'bad', 'm', 'kay', 'visually', 'impressive', 'but', 'of', 'course', 'this', 'is', 'all', 'about', 'michael', 'jackson', 'so', 'unless', 'you', 'remotely', 'like', 'mj', 'in', 'anyway', 'then', 'you', 'are', 'going', 'to', 'hate', 'this', 'and', 'find', 'it', 'boring', 'some', 'may', 'call', 'mj', 'an', 'egotist', 'for', 'consenting', 'to', 'the', 'making', 'of', 'this', 'movie', 'but', 'mj', 'and', 'most', 'of', 'his', 'fans', 'would', 'say', 'that', 'he', 'made', 'it', 'for', 'the', 'fans', 'which', 'if', 'true', 'is', 'really', 'nice', 'of', 'him', 'the', 'actual', 'feature', 'film', 'bit', 'when', 'it', 'finally', 'starts', 'is', 'only', 'on', 'for', 'minutes', 'or', 'so', 'excluding', 'the', 'smooth', 'criminal', 'sequence', 'and', 'joe', 'pesci', 'is', 'convincing', 'as', 'a', 'psychopathic', 'all', 'powerful', 'drug', 'lord', 'why', 'he', 'wants', 'mj', 'dead', 'so', 'bad', 'is', 'beyond', 'me', 'because', 'mj', 'overheard', 'his', 'plans', 'nah', 'joe', 'pesci', 's', 'character', 'ranted', 'that', 'he', 'wanted', 'people', 'to', 'know', 'it', 'is', 'he', 'who', 'is', 'supplying', 'drugs', 'etc', 'so', 'i', 'dunno', 'maybe', 'he', 'just', 'hates', 'mj', 's', 'music', 'lots', 'of', 'cool', 'things', 'in', 'this', 'like', 'mj', 'turning', 'into', 'a', 'car', 'and', 'a', 'robot', 'and', 'the', 'whole', 'speed', 'demon', 'sequence', 'also', 'the', 'director', 'must', 'have', 'had', 'the', 'patience', 'of', 'a', 'saint', 'when', 'it', 'came', 'to', 'filming', 'the', 'kiddy', 'bad', 'sequence', 'as', 'usually', 'directors', 'hate', 'working', 'with', 'one', 'kid', 'let', 'alone', 'a', 'whole', 'bunch', 'of', 'them', 'performing', 'a', 'complex', 'dance', 'scene', 'bottom', 'line', 'this', 'movie', 'is', 'for', 'people', 'who', 'like', 'mj', 'on', 'one', 'level', 'or', 'another', 'which', 'i', 'think', 'is', 'most', 'people', 'if', 'not', 'then', 'stay', 'away', 'it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl', 'michael', 'jackson', 'is', 'truly', 'one', 'of', 'the', 'most', 'talented', 'people', 'ever', 'to', 'grace', 'this', 'planet', 'but', 'is', 'he', 'guilty', 'well', 'with', 'all', 'the', 'attention', 'i', 've', 'gave', 'this', 'subject', 'hmmm', 'well', 'i', 'don', 't', 'know', 'because', 'people', 'can', 'be', 'different', 'behind', 'closed', 'doors', 'i', 'know', 'this', 'for', 'a', 'fact', 'he', 'is', 'either', 'an', 'extremely', 'nice', 'but', 'stupid', 'guy', 'or', 'one', 'of', 'the', 'most', 'sickest', 'liars', 'i', 'hope', 'he', 'is', 'not', 'the', 'latter']\n"]}],"source":["# Use reguar expressions to remove all non-letter characters,\n","letters_only = re.sub(\"[^a-zA-Z]\",\" \", sample_1.get_text() )\n","\n","# Split the review into words of only lower case.\n","words = (letters_only.lower()).split() \n","\n","# Display results\n","print(words)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we remove stop words. These are words commonly found in the language being analysed, for example, 'a' or 'the' within the English language. When considering sentiment analysis, it is logical to assume that such words would not effect the classification of the sentitment, as 'a' and 'the' offers no clue as to how the reviewer felt. For the case of sentiment analysis, removing stop words is therefore a logical pre-processing step.\n","\n","To remove stop words, we use Python's NLTK (Natural Language Toolkit) library. From this, we can get a list of stop words for the English language, which we can apply to our reviews."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:01.852134Z","iopub.status.busy":"2024-03-17T13:28:01.851692Z","iopub.status.idle":"2024-03-17T13:28:02.315683Z","shell.execute_reply":"2024-03-17T13:28:02.314488Z","shell.execute_reply.started":"2024-03-17T13:28:01.852093Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\craig\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import nltk\n","\n","# Download stop words\n","nltk.download(\"stopwords\") \n","\n","# Import stop words list\n","from nltk.corpus import stopwords\n","\n","# Extract English stop words\n","stop_words = stopwords.words(\"english\")\n","\n","# Examine effect of filtering out stop words on first review.\n","filtered_review = [word for word in words if word not in stop_words]\n","print(filtered_review)"]},{"cell_type":"markdown","metadata":{},"source":["With these pre-processing steps, the review is transformed from a prose containing HTML tags and punctuation to a collection of words which may offer insight into the sentiment. \n","\n","These steps form the pipeline for pre-processing the data. We place them into a function which is then applied to all the reviews within the training dataset."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:02.317624Z","iopub.status.busy":"2024-03-17T13:28:02.317211Z","iopub.status.idle":"2024-03-17T13:28:02.326324Z","shell.execute_reply":"2024-03-17T13:28:02.325129Z","shell.execute_reply.started":"2024-03-17T13:28:02.317593Z"},"trusted":true},"outputs":[],"source":["def review2words(raw_review, filter_stopwords = False):\n","    \"\"\"\n","    Converts a raw review, which may contain punctuation, HTML tags,\n","    varying case and more, into a collection of meaningful words to\n","    enable sentiment analysis.\n","    \n","    Parameters\n","    ----------\n","    raw_review : string\n","        String containing the review.\n","        \n","    filter_stopwords : Bool, optional\n","        Determines whether to remove stop words.\n","        Default is False.\n","        \n","    Returns\n","    -------\n","    processed_review : string\n","        String containing meaningful words.\n","    \"\"\"\n","    \n","    # Remove HTML tags\n","    review_text = BeautifulSoup(raw_review).get_text() \n","\n","    #  Remove punctuation  \n","    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n","    \n","    # Convert to lower case and separate into words\n","    words = letters_only.lower().split()                             \n","    \n","    # Filter out stop words\n","    if filter_stopwords:\n","        stops = set(stopwords.words(\"english\"))                  \n","        words = [w for w in words if not w in stops] \n","    \n","    # Return meaningful words, rejoined into a string.\n","    return( \" \".join(words))   "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:02.329012Z","iopub.status.busy":"2024-03-17T13:28:02.328251Z","iopub.status.idle":"2024-03-17T13:28:18.942133Z","shell.execute_reply":"2024-03-17T13:28:18.940644Z","shell.execute_reply.started":"2024-03-17T13:28:02.328962Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\craig\\AppData\\Local\\Temp\\ipykernel_8152\\3125727198.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]}],"source":["# Define list to hold processed reviews\n","processed_train_reviews = []\n","\n","# Loop over every review, processing and appending it to the list.\n","for idx, row in train.iterrows():\n","    \n","    try:\n","        processed_train_reviews.append(review2words(row[\"review\"], filter_stopwords = True))\n","    except:\n","        print(f'Bad review at index {idx}.')"]},{"cell_type":"markdown","metadata":{},"source":["Using the processed reviews, we are now ready to make a 'Bag of Words', which will act as the features for our sentiment analysis model. For this, we use Sci-Kit Leanrn's CountVectorizer. This takes the list of processed reviews and transforms each into a sparse vector which contains a count of how many times each word in the vocabulary was found in the review. This method removes any word order, hence the final model will only consider the words which appear and their frequency.\n","\n","Note that CountVectorizer can perform many of the pre-processing steps implemented manually above, such as converting the words to lower case. Here, they have been implemented manually in order to fully understand the underlying methodologies."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:18.944536Z","iopub.status.busy":"2024-03-17T13:28:18.944117Z","iopub.status.idle":"2024-03-17T13:28:22.403311Z","shell.execute_reply":"2024-03-17T13:28:22.402030Z","shell.execute_reply.started":"2024-03-17T13:28:18.944504Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating the bag of words...\n","\n","Bag of words has been created!\n","['abandoned' 'abc' 'abilities' ... 'zombie' 'zombies' 'zone']\n"]}],"source":["# Import CountVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","# Create an instance of CountVectorizer.\n","# Note the reviews have been pre-processed, hence many arguments are\n","# set to None.\n","# The maximum number of words to consider in the vocabulary is 5000,\n","# to limit the dimensionality of the problem.The vocabulary will only\n","# contain the most frequently occurring 5000 words in all the reviews.\n","vectoriser = CountVectorizer(analyzer = \"word\", tokenizer = None,\n","                             preprocessor = None, stop_words = None, \n","                             max_features = 5000) \n","\n","print(\"Creating the bag of words...\\n\")\n","\n","# Determine the vocabularly by fitting the CountVectorizer, then\n","# apply the CountVectorizer to the pre-processed reviews.\n","train_data_features = vectoriser.fit_transform(processed_train_reviews)\n","\n","# The output of CountVectorizer is a sparse array. We convert the output \n","# back to an array, as these are easier to work with.\n","train_data_features = train_data_features.toarray()\n","\n","print(f'Bag of words has been created!')\n","\n","# Finally, investigate the vocabulary.\n","# Take a look at the words in the vocabulary\n","vocab = vectoriser.get_feature_names_out()\n","print(vocab)"]},{"cell_type":"markdown","metadata":{},"source":["The reviews within the original training dataset have now been transformed into vectors of numbers which can be interpreted by a model. With that, a model for sentiment analysis can now be trained. We will use a random forest here."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:28:22.405692Z","iopub.status.busy":"2024-03-17T13:28:22.405185Z","iopub.status.idle":"2024-03-17T13:29:05.489180Z","shell.execute_reply":"2024-03-17T13:29:05.487815Z","shell.execute_reply.started":"2024-03-17T13:28:22.405650Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Random Forest Model...\n","Model Training Complete!\n","Accuracy = 1.0\n"]}],"source":["# Create a random forest instance with 100 trees.\n","forest = RandomForestClassifier(n_estimators = 100) \n","\n","# Fit the model to the training dataset.\n","print('Training Random Forest Model...')\n","forest = forest.fit(train_data_features, train[\"sentiment\"])\n","print('Model Training Complete!')\n","\n","# Make predictions on the training dataset.\n","predictions = forest.predict(train_data_features)\n","\n","# Calculate the accuracy of the predictions.\n","correct = predictions == train[\"sentiment\"]\n","print(f'Accuracy = {sum(correct)/len(train)}')"]},{"cell_type":"markdown","metadata":{},"source":["The accuracy on the training dataset is 100%, which may indicate that the model is overfitting. We can confirm this by analysing the performance of the model on the validation dataset."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:29:05.490768Z","iopub.status.busy":"2024-03-17T13:29:05.490438Z","iopub.status.idle":"2024-03-17T13:29:14.478846Z","shell.execute_reply":"2024-03-17T13:29:14.477357Z","shell.execute_reply.started":"2024-03-17T13:29:05.490740Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\craig\\AppData\\Local\\Temp\\ipykernel_8152\\3125727198.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]},{"name":"stdout","output_type":"stream","text":["Validation Accuracy is 0.84\n"]}],"source":["# Clean test reviews\n","clean_validate_reviews = []\n","\n","for idx, row in validate.iterrows():\n","    \n","    try:\n","        clean_validate_reviews.append(review2words(row[\"review\"], filter_stopwords = True))\n","    except:\n","        print(f'Bad review at {idx}')\n","\n","# Transform to a Bag of Words\n","processed_validate_reviews = vectoriser.transform(clean_validate_reviews)\n","\n","# Make predictions on test dataset\n","validate_predictions = forest.predict(processed_validate_reviews)\n","\n","# Calculate accuracy\n","validate_accuracy = (sum(validate_predictions == validate['sentiment'])/len(validate_predictions))\n","print(f'Validation Accuracy is {validate_accuracy:.2f}')"]},{"cell_type":"markdown","metadata":{},"source":["As the accuracy on the validation dataset is lower than that on the training dataset, it is indeed possible that the model is overfitting. This could be remedied by reducing the number of trees within the forest or applying regularisation techniques such as limiting the depth of trees, number of leaf nodes, minimum number of samples for a node to split and more.\n","\n","Despite this, the accuracy of the model is still good, particularly when compared to a simple benchmark. The original dataset contained approximately 50% positive and 50% negative samples, so if a model simply output that every review was, for example, positive, it would get it right 50% of the time. An increase in accuracy of 35% on this is good. \n","\n","We can understand a little better where the model went wrong by considering the classification report."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:29:14.482454Z","iopub.status.busy":"2024-03-17T13:29:14.482097Z","iopub.status.idle":"2024-03-17T13:29:14.511371Z","shell.execute_reply":"2024-03-17T13:29:14.509936Z","shell.execute_reply.started":"2024-03-17T13:29:14.482426Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.84      0.85      0.84      3739\n","           1       0.85      0.83      0.84      3761\n","\n","    accuracy                           0.84      7500\n","   macro avg       0.84      0.84      0.84      7500\n","weighted avg       0.84      0.84      0.84      7500\n","\n"]}],"source":["# Display classification report\n","print(classification_report(validate['sentiment'],validate_predictions))"]},{"cell_type":"markdown","metadata":{},"source":["The precision and recall of the model are fairly even, resulting in a similar F1-Score. A similar precision and recall score indicates that the model is neither overpredicting positives nor comparitively overpredicting negatives. In that case, we would expect the confusion matrix to have a fairly even number of incorrect classifications for both false positive and false negative. We can investigate this by calculating the confusion matrix."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:29:14.516594Z","iopub.status.busy":"2024-03-17T13:29:14.516231Z","iopub.status.idle":"2024-03-17T13:29:14.526734Z","shell.execute_reply":"2024-03-17T13:29:14.525532Z","shell.execute_reply.started":"2024-03-17T13:29:14.516565Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[3180  628]\n"," [ 559 3133]]\n"]}],"source":["# Calculate the confusion matrix\n","confusion = confusion_matrix(validate_predictions, validate[\"sentiment\"])\n","\n","# Print the confusion matrix\n","print(confusion)"]},{"cell_type":"markdown","metadata":{},"source":["As expected from the precision and recall scores, the model misclassifies samples evenly, indicating no bias. To improve the accuracy, we would need to investigate the misclassified samples in more detail. We could investigate\n","* if the misclassified samples contained similar words\n","* if the misclassified samples contained words not found in the vocabulary\n","\n","In the latter case, it may be that the vocabulary is too small or that the training data did not have a diverse enough vocabulary for generalisation. This could be resolved by increasing the vocabulary size and by investigating the most commonly found words within the training dataset. In the former case, it may be that the model has learnt to interpret a word incorrectly or that the word is ambiguous. For example, the reviewer may say the movie was 'sick', which to a human could have the very different meanings of 'great' or 'disturbing'. In such a case, the model may benefit from introduction of word order in the tokenisation, thus allowing it to learn context.\n","\n","However, rather than improve upon this model, we next investigate the use of another tokenisation method: word embeddings."]},{"cell_type":"markdown","metadata":{},"source":["<h2>Word2Vec</h2>\n","\n","The Bag of Words representation of text leads to large sparse vectors. Such vectors are memory intensive whilst simulataneously consisting of mostly unhelpful information - zeros. A denser representation of text which is more memory efficient is therefore desirable.\n","\n","Such a represenation is found in word embeddings. These are dense vectors which represent words in a much smaller vector space, for example 512 or 1024 numbers as opposed to the 5000 used in the previous Bag of Words example. In addition, word embeddings contain more information about the context of the word, by producing similar embeddings for similar words like great or amazing, and allow for relationships between words. For example, the vector which takes King to Queen may be the same as that which takes Boy to Girl.\n","\n","Word embeddings are themselves shallow neural networks which may be trained for a specific task or pretrained on a general task. A common example is Google's Word2Vec, which is used here. Word2Vec is a popular and efficient word embedding model. It utilises self-supervised training, by taking sentences and using each word to predict others around it, using one of two methods\n","\n","    * Continuous Bag of Words - the model uses the surrounding words to predict the middle word\n","    \n","    * Skip-Gram - the model uses the middle word to predict the words around it\n","    \n","Using these methods, Word2Vec learns to associate words used in similar contexts. \n","\n","Whilst a more efficient representation of words in terms of memory, word embeddings can be slow to train, due to the number of parameters within the models - each unique word in the training dataset is associated with multiple weights which can leads to hundreds of millions of parameters for large vocabularies. Word2Vec speeds up the training process using negative sampling, in which only a few target and non-target words are considered at each training step. This results in only a few hundred weights being updated in each training step, significantly increasing the training efficiency.\n","\n","In this section, we investigate how using a word embedding differs from the Bag of Words method and how it affects the accuracy and training of the resulting sentiment analysis model. \n","\n","To begin, we need to process the movie review texts and fit Word2Vec to them. As Word2Vec uses self-supervised learning, we do not need a labelled dataset. Because of this, we can train Word2Vec on a much larger dataset."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:29:14.529319Z","iopub.status.busy":"2024-03-17T13:29:14.528261Z","iopub.status.idle":"2024-03-17T13:29:15.415248Z","shell.execute_reply":"2024-03-17T13:29:15.413878Z","shell.execute_reply.started":"2024-03-17T13:29:14.529285Z"},"trusted":true},"outputs":[],"source":["# Read in unlabelled training data \n","unlabeled_train = pd.read_csv(os.path.join(Directory, \"unlabeledTrainData.tsv\"), header=0, delimiter=\"\\t\", quoting=3)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we need to process all the text in each movie review in the labelled and unlabelled training datasets into individual sentences, to be used in the Continuous Bag of Words or Skip-Gram methods of Word2Vec.\n","\n","The Natural Language Toolkit provides a solution for this in the Punkt Tokenizer. This tokenizer divides text into sentences using an unsupervised learning algorithm to understand abbreviation words, words that start sentences and more. Such a method is necessary as reviews may not be written in good English. For example, reviewers may not have properly capitalised words or may have left out punctuation, which would prevent us simply separating by fully stops. Also, a model such as the Punkt Tokenizer recognises the differences between full stops and capital letters in names, such as Dr. Smith, and those which end/begin sentences."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:29:15.417305Z","iopub.status.busy":"2024-03-17T13:29:15.416925Z","iopub.status.idle":"2024-03-17T13:33:06.914126Z","shell.execute_reply":"2024-03-17T13:33:06.913074Z","shell.execute_reply.started":"2024-03-17T13:29:15.417273Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Parsing sentences from training set\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\craig\\AppData\\Local\\Temp\\ipykernel_8152\\3125727198.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n","C:\\Users\\craig\\AppData\\Local\\Temp\\ipykernel_8152\\3125727198.py:23: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]},{"name":"stdout","output_type":"stream","text":["Parsing sentences from unlabeled set\n"]}],"source":["# Import the tokenizer\n","import nltk.data  \n","tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n","\n","def review2sentences(review, tokenizer, filter_stopwords=False ):\n","    \"\"\"\n","    Splits a review into its constituent sentences and returns\n","    the words in each sentence.\n","    \n","    Parameters\n","    ----------\n","    review : string\n","        Review to separate into sentences.\n","    \n","    tokenizer : model\n","        Tokenizer to use to identify start and end of sentences.\n","    \n","    remove_stopwords : bool, optional\n","        Determines whether to remove stop words from the review.\n","        Default is False.\n","    \n","    Returns\n","    -------\n","    sentences : List(List(string))\n","        Each identified sentence is split into a list of its\n","        constituent words. A list of each of these lists is then\n","        returned.\n","    \"\"\"\n","\n","    # Separate review into sentences\n","    raw_sentences = tokenizer.tokenize(review.strip())\n","    \n","    # Split each sentence into its constituent words\n","    sentences = []\n","    for raw_sentence in raw_sentences:\n","        if len(raw_sentence) > 0:\n","            sentences.append(review2words(raw_sentence, filter_stopwords).split())\n","    \n","    return sentences\n","\n","# Create empty list to store all sentences in datasets.\n","sentences = []\n","\n","# Get list of words in each sentence in each review.\n","print(\"Parsing sentences from training set\")\n","for review in train[\"review\"]:\n","    sentences += review2sentences(review, tokenizer)\n","\n","print(\"Parsing sentences from unlabeled set\")\n","for review in unlabeled_train[\"review\"]:\n","    sentences += review2sentences(review, tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["We now have a large dataset consisting of words in each sentence of our datasets. Using these, we can then train Word2Vec."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:33:06.916650Z","iopub.status.busy":"2024-03-17T13:33:06.915762Z","iopub.status.idle":"2024-03-17T13:35:08.366106Z","shell.execute_reply":"2024-03-17T13:35:08.364836Z","shell.execute_reply.started":"2024-03-17T13:33:06.916607Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-17 17:10:12,289 : INFO : collecting all words and their counts\n","2024-03-17 17:10:12,290 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2024-03-17 17:10:12,319 : INFO : PROGRESS: at sentence #10000, processed 225664 words, keeping 17775 word types\n","2024-03-17 17:10:12,349 : INFO : PROGRESS: at sentence #20000, processed 451738 words, keeping 24945 word types\n","2024-03-17 17:10:12,379 : INFO : PROGRESS: at sentence #30000, processed 670859 words, keeping 30027 word types\n","2024-03-17 17:10:12,410 : INFO : PROGRESS: at sentence #40000, processed 896841 words, keeping 34335 word types\n","2024-03-17 17:10:12,440 : INFO : PROGRESS: at sentence #50000, processed 1116082 words, keeping 37751 word types\n","2024-03-17 17:10:12,471 : INFO : PROGRESS: at sentence #60000, processed 1337544 words, keeping 40711 word types\n"]},{"name":"stdout","output_type":"stream","text":["Training model...\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-17 17:10:12,502 : INFO : PROGRESS: at sentence #70000, processed 1560307 words, keeping 43311 word types\n","2024-03-17 17:10:12,541 : INFO : PROGRESS: at sentence #80000, processed 1779516 words, keeping 45707 word types\n","2024-03-17 17:10:12,586 : INFO : PROGRESS: at sentence #90000, processed 2003714 words, keeping 48121 word types\n","2024-03-17 17:10:12,623 : INFO : PROGRESS: at sentence #100000, processed 2225465 words, keeping 50190 word types\n","2024-03-17 17:10:12,663 : INFO : PROGRESS: at sentence #110000, processed 2444323 words, keeping 52058 word types\n","2024-03-17 17:10:12,699 : INFO : PROGRESS: at sentence #120000, processed 2666488 words, keeping 54098 word types\n","2024-03-17 17:10:12,736 : INFO : PROGRESS: at sentence #130000, processed 2892315 words, keeping 55837 word types\n","2024-03-17 17:10:12,770 : INFO : PROGRESS: at sentence #140000, processed 3104796 words, keeping 57324 word types\n","2024-03-17 17:10:12,802 : INFO : PROGRESS: at sentence #150000, processed 3330432 words, keeping 59045 word types\n","2024-03-17 17:10:12,832 : INFO : PROGRESS: at sentence #160000, processed 3552466 words, keeping 60581 word types\n","2024-03-17 17:10:12,864 : INFO : PROGRESS: at sentence #170000, processed 3776048 words, keeping 62050 word types\n","2024-03-17 17:10:12,895 : INFO : PROGRESS: at sentence #180000, processed 3996237 words, keeping 63483 word types\n","2024-03-17 17:10:12,927 : INFO : PROGRESS: at sentence #190000, processed 4223084 words, keeping 64912 word types\n","2024-03-17 17:10:12,966 : INFO : PROGRESS: at sentence #200000, processed 4448683 words, keeping 66750 word types\n","2024-03-17 17:10:13,000 : INFO : PROGRESS: at sentence #210000, processed 4671060 words, keeping 68469 word types\n","2024-03-17 17:10:13,033 : INFO : PROGRESS: at sentence #220000, processed 4897384 words, keeping 70060 word types\n","2024-03-17 17:10:13,064 : INFO : PROGRESS: at sentence #230000, processed 5121948 words, keeping 71559 word types\n","2024-03-17 17:10:13,097 : INFO : PROGRESS: at sentence #240000, processed 5346087 words, keeping 73069 word types\n","2024-03-17 17:10:13,129 : INFO : PROGRESS: at sentence #250000, processed 5569577 words, keeping 74480 word types\n","2024-03-17 17:10:13,163 : INFO : PROGRESS: at sentence #260000, processed 5798907 words, keeping 75916 word types\n","2024-03-17 17:10:13,197 : INFO : PROGRESS: at sentence #270000, processed 6020467 words, keeping 77168 word types\n","2024-03-17 17:10:13,234 : INFO : PROGRESS: at sentence #280000, processed 6241837 words, keeping 78493 word types\n","2024-03-17 17:10:13,268 : INFO : PROGRESS: at sentence #290000, processed 6467626 words, keeping 79680 word types\n","2024-03-17 17:10:13,301 : INFO : PROGRESS: at sentence #300000, processed 6693052 words, keeping 81003 word types\n","2024-03-17 17:10:13,336 : INFO : PROGRESS: at sentence #310000, processed 6922754 words, keeping 82203 word types\n","2024-03-17 17:10:13,371 : INFO : PROGRESS: at sentence #320000, processed 7146365 words, keeping 83295 word types\n","2024-03-17 17:10:13,407 : INFO : PROGRESS: at sentence #330000, processed 7366184 words, keeping 84384 word types\n","2024-03-17 17:10:13,443 : INFO : PROGRESS: at sentence #340000, processed 7587339 words, keeping 85472 word types\n","2024-03-17 17:10:13,477 : INFO : PROGRESS: at sentence #350000, processed 7813962 words, keeping 86609 word types\n","2024-03-17 17:10:13,515 : INFO : PROGRESS: at sentence #360000, processed 8039726 words, keeping 87671 word types\n","2024-03-17 17:10:13,548 : INFO : PROGRESS: at sentence #370000, processed 8265115 words, keeping 88870 word types\n","2024-03-17 17:10:13,582 : INFO : PROGRESS: at sentence #380000, processed 8496344 words, keeping 90007 word types\n","2024-03-17 17:10:13,615 : INFO : PROGRESS: at sentence #390000, processed 8723341 words, keeping 90907 word types\n","2024-03-17 17:10:13,647 : INFO : PROGRESS: at sentence #400000, processed 8948148 words, keeping 91920 word types\n","2024-03-17 17:10:13,679 : INFO : PROGRESS: at sentence #410000, processed 9171231 words, keeping 93010 word types\n","2024-03-17 17:10:13,712 : INFO : PROGRESS: at sentence #420000, processed 9393631 words, keeping 93933 word types\n","2024-03-17 17:10:13,746 : INFO : PROGRESS: at sentence #430000, processed 9616999 words, keeping 94965 word types\n","2024-03-17 17:10:13,778 : INFO : PROGRESS: at sentence #440000, processed 9841818 words, keeping 95917 word types\n","2024-03-17 17:10:13,811 : INFO : PROGRESS: at sentence #450000, processed 10068762 words, keeping 96826 word types\n","2024-03-17 17:10:13,845 : INFO : PROGRESS: at sentence #460000, processed 10290074 words, keeping 97715 word types\n","2024-03-17 17:10:13,878 : INFO : PROGRESS: at sentence #470000, processed 10513702 words, keeping 98650 word types\n","2024-03-17 17:10:13,911 : INFO : PROGRESS: at sentence #480000, processed 10736383 words, keeping 99560 word types\n","2024-03-17 17:10:13,946 : INFO : PROGRESS: at sentence #490000, processed 10963694 words, keeping 100396 word types\n","2024-03-17 17:10:13,980 : INFO : PROGRESS: at sentence #500000, processed 11187656 words, keeping 101310 word types\n","2024-03-17 17:10:14,015 : INFO : PROGRESS: at sentence #510000, processed 11411178 words, keeping 102185 word types\n","2024-03-17 17:10:14,048 : INFO : PROGRESS: at sentence #520000, processed 11634300 words, keeping 103011 word types\n","2024-03-17 17:10:14,082 : INFO : PROGRESS: at sentence #530000, processed 11856720 words, keeping 103923 word types\n","2024-03-17 17:10:14,115 : INFO : PROGRESS: at sentence #540000, processed 12081387 words, keeping 104710 word types\n","2024-03-17 17:10:14,148 : INFO : PROGRESS: at sentence #550000, processed 12304662 words, keeping 105503 word types\n","2024-03-17 17:10:14,184 : INFO : PROGRESS: at sentence #560000, processed 12525720 words, keeping 106354 word types\n","2024-03-17 17:10:14,217 : INFO : PROGRESS: at sentence #570000, processed 12751443 words, keeping 107176 word types\n","2024-03-17 17:10:14,251 : INFO : PROGRESS: at sentence #580000, processed 12973612 words, keeping 107938 word types\n","2024-03-17 17:10:14,284 : INFO : PROGRESS: at sentence #590000, processed 13198067 words, keeping 108719 word types\n","2024-03-17 17:10:14,320 : INFO : PROGRESS: at sentence #600000, processed 13423356 words, keeping 109486 word types\n","2024-03-17 17:10:14,352 : INFO : PROGRESS: at sentence #610000, processed 13644023 words, keeping 110207 word types\n","2024-03-17 17:10:14,386 : INFO : PROGRESS: at sentence #620000, processed 13872334 words, keeping 111071 word types\n","2024-03-17 17:10:14,420 : INFO : PROGRESS: at sentence #630000, processed 14094824 words, keeping 111798 word types\n","2024-03-17 17:10:14,453 : INFO : PROGRESS: at sentence #640000, processed 14321319 words, keeping 112474 word types\n","2024-03-17 17:10:14,488 : INFO : PROGRESS: at sentence #650000, processed 14544819 words, keeping 113216 word types\n","2024-03-17 17:10:14,522 : INFO : PROGRESS: at sentence #660000, processed 14766837 words, keeping 113955 word types\n","2024-03-17 17:10:14,558 : INFO : PROGRESS: at sentence #670000, processed 14986880 words, keeping 114627 word types\n","2024-03-17 17:10:14,592 : INFO : PROGRESS: at sentence #680000, processed 15205740 words, keeping 115290 word types\n","2024-03-17 17:10:14,627 : INFO : PROGRESS: at sentence #690000, processed 15431608 words, keeping 116079 word types\n","2024-03-17 17:10:14,663 : INFO : PROGRESS: at sentence #700000, processed 15661051 words, keeping 116834 word types\n","2024-03-17 17:10:14,701 : INFO : PROGRESS: at sentence #710000, processed 15888061 words, keeping 117531 word types\n","2024-03-17 17:10:14,725 : INFO : collected 118079 word types from a corpus of 16046215 raw words and 717057 sentences\n","2024-03-17 17:10:14,725 : INFO : Creating a fresh vocabulary\n","2024-03-17 17:10:14,775 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 retains 15587 unique words (13.20% of original 118079, drops 102492)', 'datetime': '2024-03-17T17:10:14.775217', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n","2024-03-17 17:10:14,775 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=40 leaves 15509472 word corpus (96.66% of original 16046215, drops 536743)', 'datetime': '2024-03-17T17:10:14.775217', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n","2024-03-17 17:10:14,821 : INFO : deleting the raw counts dictionary of 118079 items\n","2024-03-17 17:10:14,823 : INFO : sample=0.001 downsamples 48 most-common words\n","2024-03-17 17:10:14,824 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 11459428.24611775 word corpus (73.9%% of prior 15509472)', 'datetime': '2024-03-17T17:10:14.824261', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n","2024-03-17 17:10:14,896 : INFO : estimated required memory for 15587 words and 300 dimensions: 45202300 bytes\n","2024-03-17 17:10:14,896 : INFO : resetting layer weights\n","2024-03-17 17:10:14,915 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-03-17T17:10:14.915728', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n","2024-03-17 17:10:14,916 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 15587 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2024-03-17T17:10:14.916728', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n","2024-03-17 17:10:15,921 : INFO : EPOCH 0 - PROGRESS: at 18.75% examples, 2127176 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:16,922 : INFO : EPOCH 0 - PROGRESS: at 38.54% examples, 2196532 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:17,927 : INFO : EPOCH 0 - PROGRESS: at 57.81% examples, 2201940 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:18,928 : INFO : EPOCH 0 - PROGRESS: at 77.79% examples, 2223562 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:19,929 : INFO : EPOCH 0 - PROGRESS: at 97.56% examples, 2231838 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:20,061 : INFO : EPOCH 0: training on 16046215 raw words (11460622 effective words) took 5.1s, 2229049 effective words/s\n","2024-03-17 17:10:21,068 : INFO : EPOCH 1 - PROGRESS: at 19.26% examples, 2182345 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:22,070 : INFO : EPOCH 1 - PROGRESS: at 38.08% examples, 2170079 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:23,071 : INFO : EPOCH 1 - PROGRESS: at 56.93% examples, 2169467 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:24,075 : INFO : EPOCH 1 - PROGRESS: at 75.77% examples, 2165605 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:25,076 : INFO : EPOCH 1 - PROGRESS: at 94.88% examples, 2168661 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:25,339 : INFO : EPOCH 1: training on 16046215 raw words (11458918 effective words) took 5.3s, 2173391 effective words/s\n","2024-03-17 17:10:26,343 : INFO : EPOCH 2 - PROGRESS: at 18.38% examples, 2087289 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:27,343 : INFO : EPOCH 2 - PROGRESS: at 36.77% examples, 2097929 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:28,345 : INFO : EPOCH 2 - PROGRESS: at 55.63% examples, 2121916 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:29,346 : INFO : EPOCH 2 - PROGRESS: at 74.98% examples, 2145346 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:30,347 : INFO : EPOCH 2 - PROGRESS: at 94.35% examples, 2159869 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:30,626 : INFO : EPOCH 2: training on 16046215 raw words (11459148 effective words) took 5.3s, 2168521 effective words/s\n","2024-03-17 17:10:31,631 : INFO : EPOCH 3 - PROGRESS: at 19.33% examples, 2192197 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:32,633 : INFO : EPOCH 3 - PROGRESS: at 39.03% examples, 2223995 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:33,634 : INFO : EPOCH 3 - PROGRESS: at 58.64% examples, 2234661 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:34,636 : INFO : EPOCH 3 - PROGRESS: at 78.23% examples, 2236607 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:35,645 : INFO : EPOCH 3 - PROGRESS: at 97.75% examples, 2232989 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:35,762 : INFO : EPOCH 3: training on 16046215 raw words (11457523 effective words) took 5.1s, 2232730 effective words/s\n","2024-03-17 17:10:36,767 : INFO : EPOCH 4 - PROGRESS: at 19.76% examples, 2241596 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:37,767 : INFO : EPOCH 4 - PROGRESS: at 39.16% examples, 2232835 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:38,768 : INFO : EPOCH 4 - PROGRESS: at 58.82% examples, 2243833 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:39,768 : INFO : EPOCH 4 - PROGRESS: at 78.60% examples, 2249537 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:40,770 : INFO : EPOCH 4 - PROGRESS: at 98.24% examples, 2249572 words/s, in_qsize 7, out_qsize 0\n","2024-03-17 17:10:40,858 : INFO : EPOCH 4: training on 16046215 raw words (11459777 effective words) took 5.1s, 2250361 effective words/s\n","2024-03-17 17:10:40,859 : INFO : Word2Vec lifecycle event {'msg': 'training on 80231075 raw words (57295988 effective words) took 25.9s, 2208580 effective words/s', 'datetime': '2024-03-17T17:10:40.859768', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n","2024-03-17 17:10:40,859 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=15587, vector_size=300, alpha=0.025>', 'datetime': '2024-03-17T17:10:40.859768', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n","2024-03-17 17:10:40,859 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'trained_Word2Vec', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2024-03-17T17:10:40.859768', 'gensim': '4.3.2', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'saving'}\n","2024-03-17 17:10:40,860 : INFO : not storing attribute cum_table\n"]},{"name":"stdout","output_type":"stream","text":["Model trained!\n"]},{"name":"stderr","output_type":"stream","text":["2024-03-17 17:10:41,212 : INFO : saved trained_Word2Vec\n"]}],"source":["# Configure logging module to enable easy to read messages from\n","# Word2Vec.\n","import logging\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n","    level=logging.INFO)\n","\n","# Set values for various parameters\n","num_features = 300    # Word vector dimensionality                      \n","min_word_count = 40   # Minimum word count                        \n","num_workers = 4       # Number of threads to run in parallel\n","context = 10          # Context window size                                                                                    \n","downsampling = 1e-3   # Downsample setting for frequent words\n","\n","# Initialize and train the model\n","from gensim.models import word2vec\n","print(\"Training model...\")\n","model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n","            vector_size=num_features, min_count = min_word_count, \\\n","            window = context, sample = downsampling)\n","print(\"Model trained!\")\n","\n","# Save trained embedding model.\n","model_name = \"trained_Word2Vec\"\n","model.save(model_name)"]},{"cell_type":"markdown","metadata":{},"source":["Before moving on to using the word embedding model, it's interesting to explore the vector space it has created. Word2Vec offers many methods to do this."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:35:08.367728Z","iopub.status.busy":"2024-03-17T13:35:08.367401Z","iopub.status.idle":"2024-03-17T13:35:08.400035Z","shell.execute_reply":"2024-03-17T13:35:08.396018Z","shell.execute_reply.started":"2024-03-17T13:35:08.367702Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-03-17 17:10:41,235 : WARNING : vectors for words {'pear', 'strawberry'} are not present in the model, ignoring these words\n"]},{"name":"stdout","output_type":"stream","text":["Most Similar Words:\n","\n","Child is most similar to: \n","teenager\n","mother\n","children\n","parent\n","kid\n","\n","Awful is most similar to: \n","terrible\n","horrible\n","atrocious\n","abysmal\n","horrendous\n","\n","Word Similarity Scores:\n","dog cat: 0.61\n","dog wolf: 0.37\n","dog land: 0.25\n","dog car: 0.37\n","dog tree: 0.48\n","\n","Words That Don't Match:\n","remote is the odd one out in horse dog sheep remote\n","hair is the odd one out in sky land hair sea\n","carrot is the odd one out in carrot apple pear strawberry\n"]}],"source":["# Look at most similar words in the vocabulary\n","print(\"Most Similar Words:\")\n","child_ms = model.wv.most_similar(\"child\", topn = 5)\n","print('\\nChild is most similar to: ')\n","for w, _ in child_ms:\n","    print(w)\n","    \n","awful_ms = model.wv.most_similar(\"awful\", topn = 5)\n","print('\\nAwful is most similar to: ')\n","for w, _ in awful_ms:\n","    print(w)\n","\n","# Get similarity scores \n","word_pairs = [\n","    (\"dog\", \"cat\"),\n","    (\"dog\", \"wolf\"),\n","    (\"dog\", \"land\"),\n","    (\"dog\", \"car\"),\n","    (\"dog\", \"tree\")\n","]\n","print(\"\\nWord Similarity Scores:\")\n","for w1, w2 in word_pairs:\n","    print(f'{w1} {w2}: {model.wv.similarity(w1, w2):.2f}')\n","\n","# Look for words which do not fit a sequence\n","sequences = [\n","    \"horse dog sheep remote\",\n","    \"sky land hair sea\",\n","    \"carrot apple pear strawberry\"\n","]\n","print(\"\\nWords That Don't Match:\")\n","for seq in sequences:\n","    print(f'{model.wv.doesnt_match(seq.split())} is the odd one out in {seq}')"]},{"cell_type":"markdown","metadata":{},"source":["The associated words look reasonable. The similarity scores for dog are interesting: dog and cat have the highest similarity, whilst dog and wolf have a low similarity. This may indicate the model has learned to associate pets. Finally, the model in general does well when looking for the word that doesn't fit with the exception of apple in carrot, apple, pear and strawberry. \n","\n","Next we look to train a model. First need to convert movie reviews into feature vectors containing the embeddings for the constituent words. To reduce the dimensionality of the problem, we consider only the average word embedding for a review, calling this our feature vector."]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:35:08.411107Z","iopub.status.busy":"2024-03-17T13:35:08.402829Z","iopub.status.idle":"2024-03-17T13:35:08.432956Z","shell.execute_reply":"2024-03-17T13:35:08.431534Z","shell.execute_reply.started":"2024-03-17T13:35:08.411012Z"},"trusted":true},"outputs":[],"source":["def words2avgfeaturevec(words, model, num_features):\n","    \"\"\"\n","    Gets the word embeddings associated with each word in\n","    a review and averages over these to get an average\n","    word embedding. This then acts as the feature vector.\n","    \n","    Parameters\n","    ----------\n","    words : List(string)\n","        List of words which appear in a single review.\n","    \n","    model : Word2Vec model\n","        Word embedding model, trained on review dataset.\n","    \n","    num_features : int\n","        Dimensionality of word embedding.\n","        \n","    Returns\n","    -------\n","    featureVec : array(double)\n","        Average word embedding over all words in the review.\n","    \"\"\"\n","\n","    # Initialise a numpy array for the final feature vector\n","    featureVec = np.zeros((num_features,),dtype=\"float32\")\n","\n","    # Set up word counter\n","    nwords = 0.\n","    \n","    # Get words in vocabulary\n","    index2word_set = set(model.wv.index_to_key)\n","    \n","    # If the word within the review is within the vocabulary,\n","    # find its word embedding and add it to the average embeddings\n","    for word in words:\n","        if word in index2word_set: \n","            nwords+=1.\n","            featureVec = np.add(featureVec,model.wv[word])\n","\n","    # Take the average\n","    featureVec = np.divide(featureVec,nwords)\n","    \n","    return featureVec\n","\n","def getAvgFeatureVecs(reviews, model, num_features):\n","    \"\"\"\n","    Converts a list of movie reviews into a 2D matrix\n","    of feature vectors, in which each review is represented\n","    as a single feature vector.\n","    \n","    Parameters\n","    ----------\n","    reviews : List(List(words))\n","        List of reviews in which each review is represented \n","        as a list of words.\n","    \n","    model : Word2Vec model\n","        Word2Vec model trained on dataset.\n","    \n","    num_features : int\n","        Dimensionality of word embeddings.\n","        \n","    Returns\n","    -------\n","    reviewFeatureVecs : array(double, double)\n","        2D array in which each row represents the average\n","        feature vector for a single review.     \n","    \"\"\"\n","    \n","    # Set up a counter\n","    counter = 0\n","    \n","    # Initialise output array\n","    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n","\n","    # Convert reviews into average feature vector\n","    for review in reviews:\n","\n","       # Output a status message every 1000th review\n","       if counter%1000 == 0:\n","           print(\"Review %d of %d\" % (counter, len(reviews)))\n","       \n","       # Make average feature vector\n","       reviewFeatureVecs[counter] = words2avgfeaturevec(review.split(), model, num_features)\n","\n","       # Increment the counter\n","       counter += 1\n","    \n","    return reviewFeatureVecs"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:35:08.435545Z","iopub.status.busy":"2024-03-17T13:35:08.434851Z","iopub.status.idle":"2024-03-17T13:36:23.195735Z","shell.execute_reply":"2024-03-17T13:36:23.194512Z","shell.execute_reply.started":"2024-03-17T13:35:08.435500Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\craig\\AppData\\Local\\Temp\\ipykernel_8152\\3125727198.py:23: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n","  review_text = BeautifulSoup(raw_review).get_text()\n"]},{"name":"stdout","output_type":"stream","text":["Creating average feature vecs for training reviews\n","Review 0 of 17500\n","Review 1000 of 17500\n","Review 2000 of 17500\n","Review 3000 of 17500\n","Review 4000 of 17500\n","Review 5000 of 17500\n","Review 6000 of 17500\n","Review 7000 of 17500\n","Review 8000 of 17500\n","Review 9000 of 17500\n","Review 10000 of 17500\n","Review 11000 of 17500\n","Review 12000 of 17500\n","Review 13000 of 17500\n","Review 14000 of 17500\n","Review 15000 of 17500\n","Review 16000 of 17500\n","Review 17000 of 17500\n","Creating average feature vecs for validating reviews\n","Review 0 of 7500\n","Review 1000 of 7500\n","Review 2000 of 7500\n","Review 3000 of 7500\n","Review 4000 of 7500\n","Review 5000 of 7500\n","Review 6000 of 7500\n","Review 7000 of 7500\n"]}],"source":["# Get movie review feature vectors for the training and validating datasets.\n","print(\"Creating average feature vecs for training reviews\")\n","clean_train_reviews = []\n","for review in train[\"review\"]:\n","    clean_train_reviews.append(review2words(review,filter_stopwords=True))\n","\n","trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n","\n","print(\"Creating average feature vecs for validating reviews\")\n","clean_validate_reviews = []\n","for review in validate[\"review\"]:\n","    clean_validate_reviews.append(review2words(review,filter_stopwords=True))\n","\n","validateDataVecs = getAvgFeatureVecs(clean_validate_reviews, model, num_features)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we fit a random forest model as before."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:36:23.197616Z","iopub.status.busy":"2024-03-17T13:36:23.197239Z","iopub.status.idle":"2024-03-17T13:36:58.968622Z","shell.execute_reply":"2024-03-17T13:36:58.967407Z","shell.execute_reply.started":"2024-03-17T13:36:23.197582Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting a random forest to the training data...\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      8761\n","           1       1.00      1.00      1.00      8739\n","\n","    accuracy                           1.00     17500\n","   macro avg       1.00      1.00      1.00     17500\n","weighted avg       1.00      1.00      1.00     17500\n","\n"]}],"source":["# Fit a random forest to the training data, using 100 trees\n","forest = RandomForestClassifier(n_estimators = 100)\n","\n","print(\"Fitting a random forest to the training data...\")\n","forest = forest.fit(trainDataVecs, train[\"sentiment\"] )\n","\n","# Get predictions on the training data\n","train_predictions = forest.predict(trainDataVecs)\n","\n","# Get the classification report for the model.\n","print(classification_report(train[\"sentiment\"], train_predictions))"]},{"cell_type":"markdown","metadata":{},"source":["The classification report suggests the model works perfectly. We can verify that with the validation set."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:36:58.970716Z","iopub.status.busy":"2024-03-17T13:36:58.970354Z","iopub.status.idle":"2024-03-17T13:36:59.197247Z","shell.execute_reply":"2024-03-17T13:36:59.196030Z","shell.execute_reply.started":"2024-03-17T13:36:58.970686Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.83      0.82      0.82      3739\n","           1       0.82      0.84      0.83      3761\n","\n","    accuracy                           0.83      7500\n","   macro avg       0.83      0.83      0.83      7500\n","weighted avg       0.83      0.83      0.83      7500\n","\n"]}],"source":["# Get predictions on the validation data\n","validate_predictions = forest.predict(validateDataVecs)\n","\n","# Get the classification report for the model.\n","print(classification_report(validate[\"sentiment\"], validate_predictions))"]},{"cell_type":"markdown","metadata":{},"source":["Unsurprisingly, the model does not perform as well on the validation dataset, which suggests that the model was heavily overfitting on the training dataset. Overall, the model performs well, though slightly worse than the Bag of Words model. \n","\n","This may be due to the averaging of the word embeddings to produce the feature vector. Taking the average of the word embeddings of each constituent word likely causes a loss of information and context, which limits what the model can learn. A representation, which utilises word embeddings but does not discard so much information, may therefore perform better.\n","\n","It is not practical to simply replace each word in the review with its embedding, due to the larger dimensions of the word embeddings compared to the individual words. Instead, we cluster words with similar meanings (which should therefore be close to one another in the word embedding space) and replace the words in each review with the cluster they are associated with. This allows us to keep a significant amount of contextual information, whilst limiting the dimensionality of the model. \n","\n","To begin, we create clusters of words using a K Means model. Such a model can take a while to train. We'll time it to get an understanding of just how long."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:36:59.200070Z","iopub.status.busy":"2024-03-17T13:36:59.199080Z","iopub.status.idle":"2024-03-17T13:51:31.139630Z","shell.execute_reply":"2024-03-17T13:51:31.138682Z","shell.execute_reply.started":"2024-03-17T13:36:59.200003Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cluster Model Training Time: 19.581807136535645 s\n"]}],"source":["from sklearn.cluster import KMeans\n","import time\n","\n","# Get start time\n","start = time.time() \n","\n","# Initially, we choose the number of clusters to be 20%\n","# of the vocabulary size.\n","word_vectors = model.wv.vectors\n","num_clusters = int(word_vectors.shape[0] / 5)\n","\n","# Train the clustering model.\n","kmeans_clustering = KMeans(n_clusters = num_clusters)\n","idx = kmeans_clustering.fit_predict(word_vectors)\n","\n","# Assess how long the model took to train\n","end = time.time()\n","elapsed = end - start\n","print(f'Cluster Model Training Time: {elapsed} s')"]},{"cell_type":"markdown","metadata":{},"source":["The model took a non-trivial amount of time to train. \n","\n","Before training a classifier model, it's interesting to investigate how words have been clustered. We can do that by finding the words that appear in the first 10 clusters."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:51:31.141997Z","iopub.status.busy":"2024-03-17T13:51:31.141304Z","iopub.status.idle":"2024-03-17T13:51:31.656665Z","shell.execute_reply":"2024-03-17T13:51:31.655528Z","shell.execute_reply.started":"2024-03-17T13:51:31.141961Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Cluster 0\n","['crystal', 'grey', 'gray', 'bell', 'babe', 'rex', 'temple', 'carrie', 'playboy', 'ma', 'valentine', 'mama', 'rosemary', 'hare']\n","\n","Cluster 1\n","['anyway', 'afterwards', 'continued', 'luckily', 'anyways', 'promised', 'correctly', 'wishing', 'sometime', 'awhile', 'compelled', 'previews', 'anticipation', 'sucker', 'canceled', 'anytime', 'dial', 'til', 'cancelled', 'eagerly', 'rewind']\n","\n","Cluster 2\n","['clean', 'split', 'beating', 'opened', 'hook', 'measure', 'fed', 'ham', 'alley', 'teams', 'screw', 'cracking', 'signed', 'hung', 'popping', 'cheer', 'rounded', 'hyped', 'downs', 'covering', 'pin', 'spice', 'cracks', 'cleaning', 'hooks', 'hurry', 'backed', 'strung', 'toss', 'ladder', 'popped', 'stir', 'sleeve', 'chopped', 'waking', 'patch', 'messing', 'teamed', 'backing', 'cracked', 'teaming', 'lined', 'hamming', 'screwing', 'wraps', 'cleaned', 'residence', 'lighten', 'pump', 'hams', 'screws', 'wrapping', 'chalk', 'messes', 'liven', 'conjure', 'winding', 'dried', 'sped', 'piling']\n","\n","Cluster 3\n","['biko']\n","\n","Cluster 4\n","['won']\n","\n","Cluster 5\n","['sentimental', 'melodramatic', 'sappy']\n","\n","Cluster 6\n","['fairbanks', 'reno', 'gable', 'billed', 'pierce', 'brosnan', 'sterling', 'hayden', 'billing', 'crowe', 'colman']\n","\n","Cluster 7\n","['safe', 'heaven', 'holy', 'apocalypse', 'forbidden', 'apes', 'mars', 'rangers', 'gods', 'battlefield', 'odyssey', 'galaxy', 'venus']\n","\n","Cluster 8\n","['insane', 'sinister', 'menacing', 'heroic', 'threatening', 'suspicious', 'incompetent', 'disguised', 'paranoid', 'active', 'dim', 'aggressive', 'blooded', 'skilled', 'antagonist', 'slimy', 'likeable', 'cunning', 'enigmatic', 'unstable', 'amoral']\n","\n","Cluster 9\n","['isn']\n"]}],"source":["# Create a dictionary which maps each word to a cluster number.     \n","word_map = dict(zip(model.wv.index_to_key, idx))\n","\n","# Find and print the words in the first 10 clusters.\n","for cluster in range(0,10):\n","    \n","    print(f'\\nCluster {cluster}')\n","    \n","    words = []\n","    for k, v in word_map.items():\n","        if(v == cluster):\n","            words.append(k)\n","    print(words)"]},{"cell_type":"markdown","metadata":{},"source":["It is immediately noticeable that the clusters are highly uneven in the number of words they contain. This may result in inefficiencies in our classifier model - if cluster X contains only one word, it is unlikely to be used frequently in the classifier model, meaning that inclusion of cluster X may add complexity with little benefit.\n","\n","In order to train a classifier model, we first need to convert each review into a list of counts indicating how often a word from that cluster appeared in the review."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:51:31.658290Z","iopub.status.busy":"2024-03-17T13:51:31.657931Z","iopub.status.idle":"2024-03-17T13:51:31.666376Z","shell.execute_reply":"2024-03-17T13:51:31.664982Z","shell.execute_reply.started":"2024-03-17T13:51:31.658260Z"},"trusted":true},"outputs":[],"source":["def words2bagofcentroids(words, word_map):\n","    \"\"\"\n","    Converts list of words which appear in a review into an array\n","    containing counts for how many times a word from each cluster\n","    was found in the review.\n","    \n","    Parameters\n","    ----------\n","    words : List(string)\n","        List of words that appear in the review.\n","    \n","    word_map : Dict(word, cluster)\n","        Dictionary in which the key is a word and the value\n","        is the cluster number that word belonged to.\n","    \n","    Returns\n","    -------\n","    bag_of_centroids : array(double)\n","        Array indicating counts for how many times a word\n","        from the cluster was found in the review.\n","    \"\"\"\n","    \n","    # Calculate the number of clusters in the word map.\n","    num_centroids = max(word_map.values()) + 1\n","\n","    # Set up output array\n","    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n","\n","    # Count the number of times a word from each cluster appears\n","    # in the review.\n","    for word in words:\n","        if word in word_map:\n","            bag_of_centroids[word_map[word]] += 1.\n","\n","    return bag_of_centroids"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:51:31.667987Z","iopub.status.busy":"2024-03-17T13:51:31.667649Z","iopub.status.idle":"2024-03-17T13:52:10.329852Z","shell.execute_reply":"2024-03-17T13:52:10.328885Z","shell.execute_reply.started":"2024-03-17T13:51:31.667958Z"},"trusted":true},"outputs":[],"source":["# Convert each review into an array of cluster counts.\n","train_clusters = np.zeros((train[\"review\"].size, num_clusters),dtype=\"float32\")\n","\n","for idx, review in enumerate(clean_train_reviews):\n","    train_clusters[idx] = words2bagofcentroids(review.split(), word_map)\n","\n","validate_clusters = np.zeros((validate[\"review\"].size, num_clusters), dtype=\"float32\")\n","\n","for idx,review in enumerate(clean_validate_reviews):\n","    validate_clusters[idx] = words2bagofcentroids(review.split(), word_map)"]},{"cell_type":"markdown","metadata":{},"source":["Now that we have cluster feature vectors, we can train a random forest model. For this model, we will use bootstrapping and the 'out-of-the-bag' score to better assess its generalisability. Bootstrapping means we train each tree in the forest on only a subset (or bag) of samples from the training dataset. The remaining samples the tree was not trained on (the out of bag samples) can then be used to assess the abilities of the tree on an unseen dataset. This method also typically achieves greater diversity in the trees within the forest. It should be noted that the sampling to get the subset is done with replacement."]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:52:10.332158Z","iopub.status.busy":"2024-03-17T13:52:10.331305Z","iopub.status.idle":"2024-03-17T13:52:52.536014Z","shell.execute_reply":"2024-03-17T13:52:52.534833Z","shell.execute_reply.started":"2024-03-17T13:52:10.332118Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting a random forest to labeled training data...\n","Random Forest Training Time = 16.052642107009888 s\n","              precision    recall  f1-score   support\n","\n","           0       0.84      0.86      0.85      3739\n","           1       0.85      0.84      0.85      3761\n","\n","    accuracy                           0.85      7500\n","   macro avg       0.85      0.85      0.85      7500\n","weighted avg       0.85      0.85      0.85      7500\n","\n","Out-of-bag score = 0.83\n"]}],"source":["# Get start time.\n","start = time.time()\n","\n","# Fit a random forest model.\n","forest = RandomForestClassifier(n_estimators = 100, bootstrap = True, oob_score = True)\n","\n","print(\"Fitting a random forest to labeled training data...\")\n","forest.fit(train_clusters,train[\"sentiment\"])\n","\n","# Output training time.\n","elapsed = time.time()-start\n","print(f'Random Forest Training Time = {elapsed} s')\n","\n","# Make predictions on the validation test set\n","validate_predictions = forest.predict(validate_clusters)\n","\n","# Output the classification report\n","print(classification_report(validate[\"sentiment\"], validate_predictions))\n","\n","# Compare accuracy of classification report to Out-Of-The-Bag score\n","print(f'Out-of-bag score = {forest.oob_score_:.2f}')"]},{"cell_type":"markdown","metadata":{},"source":["The model does a good job at classifying the sentiment of the reviews. Furthermore, the 'out-of-the-bag' score is similar to the validation accuracy, indicating that we could instead use this measure when assessing our model. This would allow us to combine the validation and training datasets, offering a larger dataset on which to train the random forest model. \n","\n","Given the very different number of words in each cluster, it's interesting to ask how important each cluster is used within the classification result. Does the model tend to use just a few clusters which have a large number of words? Or does the model use most of the clusters?\n","\n","We can investigate this by plotting the importance of each cluster within the model, where the importance of each cluster (feature) is related to the extent to which it is used to decrease the impurity."]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:52:52.539362Z","iopub.status.busy":"2024-03-17T13:52:52.537585Z","iopub.status.idle":"2024-03-17T13:52:56.377299Z","shell.execute_reply":"2024-03-17T13:52:56.375993Z","shell.execute_reply.started":"2024-03-17T13:52:52.539320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of most important clusters: 117\n"]},{"data":{"text/plain":["<matplotlib.legend.Legend at 0x12a45dd63d0>"]},"execution_count":28,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Sort the feature importances into descending order.\n","sorted_feature_importance = np.flip(np.argsort(forest.feature_importances_))\n","\n","# Create an arbitrary mask to find importances above a threshold\n","mask = forest.feature_importances_ > 0.001\n","print(f'Number of most important clusters: {sum(mask)}')\n","\n","# Plot the feature importances\n","plt.hist(forest.feature_importances_, bins=1000, density = True, label=\"All\")\n","plt.hist(forest.feature_importances_[mask], bins=1000, density = True, label = \"Most Important\")\n","plt.xlabel(\"Feature Importance\"); plt.ylabel(\"Probability Density\"); \n","plt.legend(loc = \"upper right\")"]},{"cell_type":"markdown","metadata":{},"source":["The classification model uses only a fraction of the available clusters to make its decisions. The remaining clusters, used only a few times, may be contributing to overfitting, as the model looks to memorise the training dataset. We can assess this, by retraining the model on only a few hundred clusters."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:52:56.379914Z","iopub.status.busy":"2024-03-17T13:52:56.379064Z","iopub.status.idle":"2024-03-17T13:53:02.055644Z","shell.execute_reply":"2024-03-17T13:53:02.054274Z","shell.execute_reply.started":"2024-03-17T13:52:56.379867Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Time for Random Forest Model = 2.138252019882202 s\n","Out of Bag Score: 0.82\n","              precision    recall  f1-score   support\n","\n","           0       0.83      0.83      0.83      3739\n","           1       0.83      0.83      0.83      3761\n","\n","    accuracy                           0.83      7500\n","   macro avg       0.83      0.83      0.83      7500\n","weighted avg       0.83      0.83      0.83      7500\n","\n"]}],"source":["# Reduce training feature vectors to contain only the most important\n","# clusters.\n","reduced_train_clusters = train_clusters[:,mask]\n","\n","start = time.time()\n","\n","# Create a new Random Forest model.\n","reduced_forest = RandomForestClassifier(n_estimators=100, bootstrap=True, oob_score=True)\n","\n","# Fit the new model to the training data\n","reduced_forest.fit(reduced_train_clusters, train[\"sentiment\"])\n","print(f'Training Time for Random Forest Model = {time.time()-start} s')\n","\n","# Assess model\n","print(f'Out of Bag Score: {reduced_forest.oob_score_:.2f}')\n","\n","# Predict sentiment labels for the validation set\n","reduced_validate_clusters = validate_clusters[:,mask];\n","validation_predictions = reduced_forest.predict(reduced_validate_clusters)\n","print(classification_report(validate[\"sentiment\"], validation_predictions))"]},{"cell_type":"markdown","metadata":{},"source":["Significantly reducing the number of clusters (and therefore features) within the training dataset has limited impact on the accuracy of the model, whilst reducing the training time by approximately 7 times. \n","\n","Finally, just for interest, we can look at the most important clusters and the words they contained."]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-03-17T13:53:02.057765Z","iopub.status.busy":"2024-03-17T13:53:02.057156Z","iopub.status.idle":"2024-03-17T13:53:02.171986Z","shell.execute_reply":"2024-03-17T13:53:02.170787Z","shell.execute_reply.started":"2024-03-17T13:53:02.057731Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Corresponding Words: ['bad']\n","Corresponding Words: ['worst']\n","Corresponding Words: ['great']\n","Corresponding Words: ['waste']\n","Corresponding Words: ['awful']\n","Corresponding Words: ['terrible', 'horrible']\n","Corresponding Words: ['brilliant', 'fantastic', 'superb', 'terrific', 'outstanding']\n","Corresponding Words: ['excellent']\n","Corresponding Words: ['lazy', 'stupidity', 'idiotic', 'useless', 'trite', 'cardboard', 'childish', 'meaningless', 'superficial', 'clumsy', 'inane', 'irrelevant', 'cliche', 'inducing', 'senseless', 'stilted', 'mundane', 'stale', 'illogical', 'moronic', 'nonsensical', 'banal', 'incomprehensible', 'pitiful', 'insipid', 'hackneyed', 'imaginable', 'unimaginative']\n","Corresponding Words: ['dreadful', 'atrocious', 'horrid', 'horrendous', 'unwatchable', 'worthless', 'appalling', 'abysmal']\n"]}],"source":["# Print words in 10 most important clusters.\n","keys = []\n","for c in sorted_feature_importance[:10]:\n","    words = []\n","    for key, value in word_map.items():\n","        if value == c:\n","            words.append(key)\n","\n","    print(\"Corresponding Words:\", words)"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Conclusions</h2>\n","To conclude:\n","\n","* Using the Kaggle Getting Started: Bag of Words meets Bags of Popcorn data, we have explored three methods to represent text within Natural Language Processing.\n","\n","* Firstly we explored the Bag of Words method. This worked well for the sentiment analysis investigated here, however have a large dimensionality which result in memory inefficiencies.\n","\n","* Next, we explored word embeddings. These reduce the dimensionality of the words and allow context, but processing them into feature vectors is non-trivial.\n","\n","* Initially, the average word embedding for all words in a review was used as the feature vector input to the sentiment analysis model. The resultant model performed similarly to the Bag of Words based model, though it is likely that by averaging the word embeddings, vital information was lost.\n","\n","* Then, a clustering model was used to cluster similar word embeddings. The feature vectors for the reviews were then a count of how many times a word from each cluster appeared in the review. This worked well however the model primarily used only a small number of clusters to decide the sentiment.\n","\n","* Removing the less important clusters reduced the dimensionality of the model, speeding up training, whilst having limited impact of the accuracy of the model.\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":21785,"sourceId":28006,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
